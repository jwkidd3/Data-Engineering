{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Sorting Data Frames\n",
    "\n",
    "Let us understand how to sort the data in a Data Frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use `orderBy` or `sort` to sort the data.\n",
    "* We can perform composite sorting by passing multiple columns or expressions.\n",
    "* By default data is sorted in ascending order, we can change it to descending by applying `desc()` function on the column or expression.\n",
    "* If the sort column contain null values those will come first. We can change the position of nulls to last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Basic Transformations'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = \"/public/airtraffic_all/airtraffic-part/flightmonth=200801\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic = spark. \\\n",
    "    read. \\\n",
    "    parquet(airtraffic_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get daily count of cancelled flights where data is sorted in ascending order by count of cancelled flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, lpad, lit, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|FlightDate|FlightCount|\n",
      "+----------+-----------+\n",
      "|  20080120|        247|\n",
      "|  20080130|        694|\n",
      "|  20080115|        299|\n",
      "|  20080118|        230|\n",
      "|  20080122|        788|\n",
      "|  20080104|        769|\n",
      "|  20080125|        526|\n",
      "|  20080102|        511|\n",
      "|  20080105|        456|\n",
      "|  20080111|        524|\n",
      "|  20080109|        377|\n",
      "|  20080127|        638|\n",
      "|  20080101|        552|\n",
      "|  20080128|        654|\n",
      "|  20080119|        876|\n",
      "|  20080106|        683|\n",
      "|  20080123|        530|\n",
      "|  20080117|        872|\n",
      "|  20080116|        532|\n",
      "|  20080112|        226|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('cancelled = 1'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            col(\"Year\"),\n",
    "            lpad(col(\"Month\"), 2, \"0\"),\n",
    "            lpad(col(\"DayOfMonth\"), 2, \"0\")\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|FlightDate|FlightCount|\n",
      "+----------+-----------+\n",
      "|  20080112|        226|\n",
      "|  20080118|        230|\n",
      "|  20080120|        247|\n",
      "|  20080115|        299|\n",
      "|  20080124|        322|\n",
      "|  20080110|        341|\n",
      "|  20080113|        359|\n",
      "|  20080109|        377|\n",
      "|  20080126|        416|\n",
      "|  20080105|        456|\n",
      "|  20080108|        463|\n",
      "|  20080103|        475|\n",
      "|  20080121|        475|\n",
      "|  20080102|        511|\n",
      "|  20080111|        524|\n",
      "|  20080125|        526|\n",
      "|  20080123|        530|\n",
      "|  20080116|        532|\n",
      "|  20080101|        552|\n",
      "|  20080107|        579|\n",
      "|  20080127|        638|\n",
      "|  20080128|        654|\n",
      "|  20080106|        683|\n",
      "|  20080130|        694|\n",
      "|  20080104|        769|\n",
      "|  20080122|        788|\n",
      "|  20080117|        872|\n",
      "|  20080119|        876|\n",
      "|  20080129|        889|\n",
      "|  20080114|        909|\n",
      "|  20080131|       1081|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('cancelled = 1'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            col(\"Year\"),\n",
    "            lpad(col(\"Month\"), 2, \"0\"),\n",
    "            lpad(col(\"DayOfMonth\"), 2, \"0\")\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    orderBy('FlightCount'). \\\n",
    "    show(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|FlightDate|FlightCount|\n",
      "+----------+-----------+\n",
      "|  20080112|        226|\n",
      "|  20080118|        230|\n",
      "|  20080120|        247|\n",
      "|  20080115|        299|\n",
      "|  20080124|        322|\n",
      "|  20080110|        341|\n",
      "|  20080113|        359|\n",
      "|  20080109|        377|\n",
      "|  20080126|        416|\n",
      "|  20080105|        456|\n",
      "|  20080108|        463|\n",
      "|  20080103|        475|\n",
      "|  20080121|        475|\n",
      "|  20080102|        511|\n",
      "|  20080111|        524|\n",
      "|  20080125|        526|\n",
      "|  20080123|        530|\n",
      "|  20080116|        532|\n",
      "|  20080101|        552|\n",
      "|  20080107|        579|\n",
      "|  20080127|        638|\n",
      "|  20080128|        654|\n",
      "|  20080106|        683|\n",
      "|  20080130|        694|\n",
      "|  20080104|        769|\n",
      "|  20080122|        788|\n",
      "|  20080117|        872|\n",
      "|  20080119|        876|\n",
      "|  20080129|        889|\n",
      "|  20080114|        909|\n",
      "|  20080131|       1081|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('cancelled = 1'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            col(\"Year\"),\n",
    "            lpad(col(\"Month\"), 2, \"0\"),\n",
    "            lpad(col(\"DayOfMonth\"), 2, \"0\")\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    orderBy(col('FlightCount').asc()). \\\n",
    "    show(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get daily count of cancelled flights where data is sorted in descending order by count of cancelled flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|FlightDate|FlightCount|\n",
      "+----------+-----------+\n",
      "|  20080131|       1081|\n",
      "|  20080114|        909|\n",
      "|  20080129|        889|\n",
      "|  20080119|        876|\n",
      "|  20080117|        872|\n",
      "|  20080122|        788|\n",
      "|  20080104|        769|\n",
      "|  20080130|        694|\n",
      "|  20080106|        683|\n",
      "|  20080128|        654|\n",
      "|  20080127|        638|\n",
      "|  20080107|        579|\n",
      "|  20080101|        552|\n",
      "|  20080116|        532|\n",
      "|  20080123|        530|\n",
      "|  20080125|        526|\n",
      "|  20080111|        524|\n",
      "|  20080102|        511|\n",
      "|  20080121|        475|\n",
      "|  20080103|        475|\n",
      "|  20080108|        463|\n",
      "|  20080105|        456|\n",
      "|  20080126|        416|\n",
      "|  20080109|        377|\n",
      "|  20080113|        359|\n",
      "|  20080110|        341|\n",
      "|  20080124|        322|\n",
      "|  20080115|        299|\n",
      "|  20080120|        247|\n",
      "|  20080118|        230|\n",
      "|  20080112|        226|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    filter('cancelled = 1'). \\\n",
    "    groupBy(\n",
    "        concat(\n",
    "            col(\"Year\"),\n",
    "            lpad(col(\"Month\"), 2, \"0\"),\n",
    "            lpad(col(\"DayOfMonth\"), 2, \"0\")\n",
    "        ).alias('FlightDate')\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('FlightCount')). \\\n",
    "    orderBy(col('FlightCount').desc()). \\\n",
    "    show(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Project Year, Month, DayOfMonth, CRSDepTime and Origin for this task.\n",
    "* Sort the data based up on year, month, day of month and then using scheduled time. Data should be sorted in ascending order by year, month and day of month then in descending order by scheduled time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------+------+\n",
      "|Year|Month|DayOfMonth|CRSDepTime|Origin|\n",
      "+----+-----+----------+----------+------+\n",
      "|2008|    1|        16|      1735|   BGR|\n",
      "|2008|    1|        17|      1701|   SYR|\n",
      "|2008|    1|        17|      1225|   SAV|\n",
      "|2008|    1|        17|      1530|   CVG|\n",
      "|2008|    1|        17|      1205|   STL|\n",
      "|2008|    1|        18|      1150|   STL|\n",
      "|2008|    1|        18|      1009|   MCI|\n",
      "|2008|    1|        19|       835|   TUL|\n",
      "|2008|    1|        20|      1935|   JFK|\n",
      "|2008|    1|        20|       830|   RDU|\n",
      "|2008|    1|        21|      1640|   CVG|\n",
      "|2008|    1|        21|      1204|   MSY|\n",
      "|2008|    1|        21|      1935|   JFK|\n",
      "|2008|    1|        21|      1830|   DCA|\n",
      "|2008|    1|        21|       700|   HSV|\n",
      "|2008|    1|        22|      1910|   ORD|\n",
      "|2008|    1|        22|      1320|   CVG|\n",
      "|2008|    1|        23|       908|   LGA|\n",
      "|2008|    1|        23|      1252|   CLT|\n",
      "|2008|    1|        23|       635|   GSP|\n",
      "+----+-----+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('Year', 'Month', 'DayOfMonth', 'CRSDepTime', 'Origin'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------+------+\n",
      "|Year|Month|DayOfMonth|CRSDepTime|Origin|\n",
      "+----+-----+----------+----------+------+\n",
      "|2008|    1|         1|        10|   LAX|\n",
      "|2008|    1|         1|        15|   SMF|\n",
      "|2008|    1|         1|        25|   SMF|\n",
      "|2008|    1|         1|        25|   PHX|\n",
      "|2008|    1|         1|        30|   ANC|\n",
      "|2008|    1|         1|        30|   LAX|\n",
      "|2008|    1|         1|        30|   LAS|\n",
      "|2008|    1|         1|        30|   ONT|\n",
      "|2008|    1|         1|        35|   MCO|\n",
      "|2008|    1|         1|        35|   SFO|\n",
      "|2008|    1|         1|        40|   LAX|\n",
      "|2008|    1|         1|        40|   LAS|\n",
      "|2008|    1|         1|        40|   LAX|\n",
      "|2008|    1|         1|        40|   SEA|\n",
      "|2008|    1|         1|        40|   SFO|\n",
      "|2008|    1|         1|        40|   SEA|\n",
      "|2008|    1|         1|        45|   PHX|\n",
      "|2008|    1|         1|        45|   LAS|\n",
      "|2008|    1|         1|        50|   ANC|\n",
      "|2008|    1|         1|        53|   PDX|\n",
      "+----+-----+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('Year', 'Month', 'DayOfMonth', 'CRSDepTime', 'Origin'). \\\n",
    "    orderBy('Year', 'Month', 'DayOfMonth', 'CRSDepTime'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------+------+\n",
      "|Year|Month|DayOfMonth|CRSDepTime|Origin|\n",
      "+----+-----+----------+----------+------+\n",
      "|2008|    1|         1|      2359|   PHX|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   PHX|\n",
      "|2008|    1|         1|      2359|   SLC|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   SLC|\n",
      "|2008|    1|         1|      2359|   SEA|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2359|   TUS|\n",
      "|2008|    1|         1|      2359|   LAS|\n",
      "|2008|    1|         1|      2358|   LAS|\n",
      "|2008|    1|         1|      2358|   LAS|\n",
      "|2008|    1|         1|      2356|   LAS|\n",
      "+----+-----+----------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtraffic. \\\n",
    "    select('Year', 'Month', 'DayOfMonth', 'CRSDepTime', 'Origin'). \\\n",
    "    orderBy('Year', 'Month', 'DayOfMonth', col('CRSDepTime').desc()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create employees Data Frame and get employees data in ascending order by nationality. However, data related to United States should come at top always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, 10,\n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, None,\n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, '',\n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, 2,\n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, bonus STRING, nationality STRING,\n",
    "                    phone_number STRING, ssn STRING\"\"\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
       "If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
       "\n",
       ":param condition: a boolean :class:`Column` expression.\n",
       ":param value: a literal value, or a :class:`Column` expression.\n",
       "\n",
       ">>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n",
       "[Row(age=3), Row(age=4)]\n",
       "\n",
       ">>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\n",
       "[Row(age=3), Row(age=None)]\n",
       "\n",
       ".. versionadded:: 1.4\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "when?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|sort_column|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|          0|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|          1|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|          1|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|          1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn('sort_column', when(upper(col('nationality')) == 'UNITED STATES', 0).otherwise(1)). \\\n",
    "    orderBy('sort_column', 'nationality'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|sort_column|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|          0|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|          1|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|          1|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|          1|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    withColumn(\n",
    "        'sort_column', \n",
    "        expr(\"\"\"CASE WHEN upper(nationality) = 'UNITED STATES' THEN 0 else 1 END\"\"\")\n",
    "    ). \\\n",
    "    orderBy('sort_column', 'nationality'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sort the data in employeesDF using bonus. Data should be sorted numerically and null and empty values should come at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- bonus: string (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy('bonus'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(col('bonus').cast('int')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = col('X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Column in module pyspark.sql.column object:\n",
      "\n",
      "class Column(builtins.object)\n",
      " |  A column in a DataFrame.\n",
      " |  \n",
      " |  :class:`Column` instances can be created by::\n",
      " |  \n",
      " |      # 1. Select a column out of a DataFrame\n",
      " |  \n",
      " |      df.colName\n",
      " |      df[\"colName\"]\n",
      " |  \n",
      " |      # 2. Create from an expression\n",
      " |      df.colName + 1\n",
      " |      1 / df.colName\n",
      " |  \n",
      " |  .. versionadded:: 1.3\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __and__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __bool__ = __nonzero__(self)\n",
      " |  \n",
      " |  __contains__(self, item)\n",
      " |      # container operators\n",
      " |  \n",
      " |  __div__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __eq__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ge__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __getattr__(self, item)\n",
      " |  \n",
      " |  __getitem__(self, k)\n",
      " |  \n",
      " |  __gt__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __init__(self, jc)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __invert__ = _(self)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __le__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __lt__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mod__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __mul__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ne__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __neg__ = _(self)\n",
      " |  \n",
      " |  __nonzero__(self)\n",
      " |  \n",
      " |  __or__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __pow__ = _(self, other)\n",
      " |      binary function\n",
      " |  \n",
      " |  __radd__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rand__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rdiv__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmod__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rmul__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __ror__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rpow__ = _(self, other)\n",
      " |      binary function\n",
      " |  \n",
      " |  __rsub__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __rtruediv__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __sub__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  __truediv__ = _(self, other)\n",
      " |      binary operator\n",
      " |  \n",
      " |  alias(self, *alias, **kwargs)\n",
      " |      Returns this column aliased with a new name or names (in the case of expressions that\n",
      " |      return more than one column, such as explode).\n",
      " |      \n",
      " |      :param alias: strings of desired column names (collects all positional arguments passed)\n",
      " |      :param metadata: a dict of information to be stored in ``metadata`` attribute of the\n",
      " |          corresponding :class: `StructField` (optional, keyword only argument)\n",
      " |      \n",
      " |      .. versionchanged:: 2.2\n",
      " |         Added optional ``metadata`` argument.\n",
      " |      \n",
      " |      >>> df.select(df.age.alias(\"age2\")).collect()\n",
      " |      [Row(age2=2), Row(age2=5)]\n",
      " |      >>> df.select(df.age.alias(\"age3\", metadata={'max': 99})).schema['age3'].metadata['max']\n",
      " |      99\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  asc = _(self)\n",
      " |      Returns a sort expression based on ascending order of the column.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc()).collect()\n",
      " |      [Row(name='Alice'), Row(name='Tom')]\n",
      " |  \n",
      " |  asc_nulls_first = _(self)\n",
      " |      Returns a sort expression based on ascending order of the column, and null values\n",
      " |      return before non-null values.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc_nulls_first()).collect()\n",
      " |      [Row(name=None), Row(name='Alice'), Row(name='Tom')]\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  asc_nulls_last = _(self)\n",
      " |      Returns a sort expression based on ascending order of the column, and null values\n",
      " |      appear after non-null values.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()\n",
      " |      [Row(name='Alice'), Row(name='Tom'), Row(name=None)]\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  astype = cast(self, dataType)\n",
      " |      :func:`astype` is an alias for :func:`cast`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  between(self, lowerBound, upperBound)\n",
      " |      A boolean expression that is evaluated to true if the value of this\n",
      " |      expression is between the given columns.\n",
      " |      \n",
      " |      >>> df.select(df.name, df.age.between(2, 4)).show()\n",
      " |      +-----+---------------------------+\n",
      " |      | name|((age >= 2) AND (age <= 4))|\n",
      " |      +-----+---------------------------+\n",
      " |      |Alice|                       true|\n",
      " |      |  Bob|                      false|\n",
      " |      +-----+---------------------------+\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  bitwiseAND = _(self, other)\n",
      " |      Compute bitwise AND of this expression with another expression.\n",
      " |      \n",
      " |      :param other: a value or :class:`Column` to calculate bitwise and(&) against\n",
      " |                    this :class:`Column`.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseAND(df.b)).collect()\n",
      " |      [Row((a & b)=10)]\n",
      " |  \n",
      " |  bitwiseOR = _(self, other)\n",
      " |      Compute bitwise OR of this expression with another expression.\n",
      " |      \n",
      " |      :param other: a value or :class:`Column` to calculate bitwise or(|) against\n",
      " |                    this :class:`Column`.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseOR(df.b)).collect()\n",
      " |      [Row((a | b)=235)]\n",
      " |  \n",
      " |  bitwiseXOR = _(self, other)\n",
      " |      Compute bitwise XOR of this expression with another expression.\n",
      " |      \n",
      " |      :param other: a value or :class:`Column` to calculate bitwise xor(^) against\n",
      " |                    this :class:`Column`.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(a=170, b=75)])\n",
      " |      >>> df.select(df.a.bitwiseXOR(df.b)).collect()\n",
      " |      [Row((a ^ b)=225)]\n",
      " |  \n",
      " |  cast(self, dataType)\n",
      " |      Convert the column into type ``dataType``.\n",
      " |      \n",
      " |      >>> df.select(df.age.cast(\"string\").alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |      >>> df.select(df.age.cast(StringType()).alias('ages')).collect()\n",
      " |      [Row(ages='2'), Row(ages='5')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  contains = _(self, other)\n",
      " |      Contains the other element. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      :param other: string in line\n",
      " |      \n",
      " |      >>> df.filter(df.name.contains('o')).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |  \n",
      " |  desc = _(self)\n",
      " |      Returns a sort expression based on the descending order of the column.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc()).collect()\n",
      " |      [Row(name='Tom'), Row(name='Alice')]\n",
      " |  \n",
      " |  desc_nulls_first = _(self)\n",
      " |      Returns a sort expression based on the descending order of the column, and null values\n",
      " |      appear before non-null values.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc_nulls_first()).collect()\n",
      " |      [Row(name=None), Row(name='Tom'), Row(name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  desc_nulls_last = _(self)\n",
      " |      Returns a sort expression based on the descending order of the column, and null values\n",
      " |      appear after non-null values.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      " |      >>> df.select(df.name).orderBy(df.name.desc_nulls_last()).collect()\n",
      " |      [Row(name='Tom'), Row(name='Alice'), Row(name=None)]\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  endswith = _(self, other)\n",
      " |      String ends with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      :param other: string at end of line (do not use a regex `$`)\n",
      " |      \n",
      " |      >>> df.filter(df.name.endswith('ice')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.endswith('ice$')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  eqNullSafe = _(self, other)\n",
      " |      Equality test that is safe for null values.\n",
      " |      \n",
      " |      :param other: a value or :class:`Column`\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df1 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value='foo'),\n",
      " |      ...     Row(id=2, value=None)\n",
      " |      ... ])\n",
      " |      >>> df1.select(\n",
      " |      ...     df1['value'] == 'foo',\n",
      " |      ...     df1['value'].eqNullSafe('foo'),\n",
      " |      ...     df1['value'].eqNullSafe(None)\n",
      " |      ... ).show()\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |(value = foo)|(value <=> foo)|(value <=> NULL)|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      |         true|           true|           false|\n",
      " |      |         null|          false|            true|\n",
      " |      +-------------+---------------+----------------+\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(value = 'bar'),\n",
      " |      ...     Row(value = None)\n",
      " |      ... ])\n",
      " |      >>> df1.join(df2, df1[\"value\"] == df2[\"value\"]).count()\n",
      " |      0\n",
      " |      >>> df1.join(df2, df1[\"value\"].eqNullSafe(df2[\"value\"])).count()\n",
      " |      1\n",
      " |      >>> df2 = spark.createDataFrame([\n",
      " |      ...     Row(id=1, value=float('NaN')),\n",
      " |      ...     Row(id=2, value=42.0),\n",
      " |      ...     Row(id=3, value=None)\n",
      " |      ... ])\n",
      " |      >>> df2.select(\n",
      " |      ...     df2['value'].eqNullSafe(None),\n",
      " |      ...     df2['value'].eqNullSafe(float('NaN')),\n",
      " |      ...     df2['value'].eqNullSafe(42.0)\n",
      " |      ... ).show()\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |(value <=> NULL)|(value <=> NaN)|(value <=> 42.0)|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      |           false|           true|           false|\n",
      " |      |           false|          false|            true|\n",
      " |      |            true|          false|           false|\n",
      " |      +----------------+---------------+----------------+\n",
      " |      \n",
      " |      .. note:: Unlike Pandas, PySpark doesn't consider NaN values to be NULL.\n",
      " |         See the `NaN Semantics`_ for details.\n",
      " |      .. _NaN Semantics:\n",
      " |         https://spark.apache.org/docs/latest/sql-programming-guide.html#nan-semantics\n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  getField(self, name)\n",
      " |      An expression that gets a field by name in a StructField.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
      " |      >>> df.select(df.r.getField(\"b\")).show()\n",
      " |      +---+\n",
      " |      |r.b|\n",
      " |      +---+\n",
      " |      |  b|\n",
      " |      +---+\n",
      " |      >>> df.select(df.r.a).show()\n",
      " |      +---+\n",
      " |      |r.a|\n",
      " |      +---+\n",
      " |      |  1|\n",
      " |      +---+\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  getItem(self, key)\n",
      " |      An expression that gets an item at position ``ordinal`` out of a list,\n",
      " |      or gets an item by key out of a dict.\n",
      " |      \n",
      " |      >>> df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
      " |      >>> df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()\n",
      " |      +----+------+\n",
      " |      |l[0]|d[key]|\n",
      " |      +----+------+\n",
      " |      |   1| value|\n",
      " |      +----+------+\n",
      " |      >>> df.select(df.l[0], df.d[\"key\"]).show()\n",
      " |      +----+------+\n",
      " |      |l[0]|d[key]|\n",
      " |      +----+------+\n",
      " |      |   1| value|\n",
      " |      +----+------+\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  isNotNull = _(self)\n",
      " |      True if the current expression is NOT null.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNotNull()).collect()\n",
      " |      [Row(height=80, name='Tom')]\n",
      " |  \n",
      " |  isNull = _(self)\n",
      " |      True if the current expression is null.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
      " |      >>> df.filter(df.height.isNull()).collect()\n",
      " |      [Row(height=None, name='Alice')]\n",
      " |  \n",
      " |  isin(self, *cols)\n",
      " |      A boolean expression that is evaluated to true if the value of this\n",
      " |      expression is contained by the evaluated values of the arguments.\n",
      " |      \n",
      " |      >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df.age.isin([1, 2, 3])].collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  like = _(self, other)\n",
      " |      SQL like expression. Returns a boolean :class:`Column` based on a SQL LIKE match.\n",
      " |      \n",
      " |      :param other: a SQL LIKE pattern\n",
      " |      \n",
      " |      See :func:`rlike` for a regex version\n",
      " |      \n",
      " |      >>> df.filter(df.name.like('Al%')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  name = alias(self, *alias, **kwargs)\n",
      " |      :func:`name` is an alias for :func:`alias`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  otherwise(self, value)\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      See :func:`pyspark.sql.functions.when` for example usage.\n",
      " |      \n",
      " |      :param value: a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\n",
      " |      +-----+-------------------------------------+\n",
      " |      | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
      " |      +-----+-------------------------------------+\n",
      " |      |Alice|                                    0|\n",
      " |      |  Bob|                                    1|\n",
      " |      +-----+-------------------------------------+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  over(self, window)\n",
      " |      Define a windowing column.\n",
      " |      \n",
      " |      :param window: a :class:`WindowSpec`\n",
      " |      :return: a Column\n",
      " |      \n",
      " |      >>> from pyspark.sql import Window\n",
      " |      >>> window = Window.partitionBy(\"name\").orderBy(\"age\")                 .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
      " |      >>> from pyspark.sql.functions import rank, min\n",
      " |      >>> df.withColumn(\"rank\", rank().over(window))                 .withColumn(\"min\", min('age').over(window)).show()\n",
      " |      +---+-----+----+---+\n",
      " |      |age| name|rank|min|\n",
      " |      +---+-----+----+---+\n",
      " |      |  5|  Bob|   1|  5|\n",
      " |      |  2|Alice|   1|  2|\n",
      " |      +---+-----+----+---+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  rlike = _(self, other)\n",
      " |      SQL RLIKE expression (LIKE with Regex). Returns a boolean :class:`Column` based on a regex\n",
      " |      match.\n",
      " |      \n",
      " |      :param other: an extended regex expression\n",
      " |      \n",
      " |      >>> df.filter(df.name.rlike('ice$')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  startswith = _(self, other)\n",
      " |      String starts with. Returns a boolean :class:`Column` based on a string match.\n",
      " |      \n",
      " |      :param other: string at start of line (do not use a regex `^`)\n",
      " |      \n",
      " |      >>> df.filter(df.name.startswith('Al')).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.filter(df.name.startswith('^Al')).collect()\n",
      " |      []\n",
      " |  \n",
      " |  substr(self, startPos, length)\n",
      " |      Return a :class:`Column` which is a substring of the column.\n",
      " |      \n",
      " |      :param startPos: start position (int or Column)\n",
      " |      :param length:  length of the substring (int or Column)\n",
      " |      \n",
      " |      >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n",
      " |      [Row(col='Ali'), Row(col='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  when(self, condition, value)\n",
      " |      Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      " |      If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      " |      \n",
      " |      See :func:`pyspark.sql.functions.when` for example usage.\n",
      " |      \n",
      " |      :param condition: a boolean :class:`Column` expression.\n",
      " |      :param value: a literal value, or a :class:`Column` expression.\n",
      " |      \n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      |Alice|                                                          -1|\n",
      " |      |  Bob|                                                           1|\n",
      " |      +-----+------------------------------------------------------------+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method _ in module pyspark.sql.column:\n",
      "\n",
      "_() method of pyspark.sql.column.Column instance\n",
      "    Returns a sort expression based on ascending order of the column, and null values\n",
      "    appear after non-null values.\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> df = spark.createDataFrame([('Tom', 80), (None, 60), ('Alice', None)], [\"name\", \"height\"])\n",
      "    >>> df.select(df.name).orderBy(df.name.asc_nulls_last()).collect()\n",
      "    [Row(name='Alice'), Row(name='Tom'), Row(name=None)]\n",
      "    \n",
      "    .. versionadded:: 2.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(c.asc_nulls_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|bonus|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|    2|     AUSTRALIA|+61 987 654 3210|789 12 6118|\n",
      "|          1|     Scott|    Tiger|1000.0|   10| united states| +1 123 456 7890|123 45 6789|\n",
      "|          3|      Nick|   Junior| 750.0|     |united KINGDOM|+44 111 111 1111|222 33 4444|\n",
      "|          2|     Henry|     Ford|1250.0| null|         India|+91 234 567 8901|456 78 9123|\n",
      "+-----------+----------+---------+------+-----+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF. \\\n",
    "    orderBy(employeesDF.bonus.cast('int').asc_nulls_last()). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
