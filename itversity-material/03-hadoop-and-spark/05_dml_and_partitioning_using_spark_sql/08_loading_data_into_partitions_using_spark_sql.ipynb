{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading into Partitions\n",
    "\n",
    "Let us understand how to use load command to load data into partitioned tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username = itv002480\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "itv002480"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val username = System.getProperty(\"user.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username = itv002480\n",
       "spark = org.apache.spark.sql.SparkSession@3786e5cd\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@3786e5cd"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    config(\"spark.sql.warehouse.dir\", s\"/user/${username}/warehouse\").\n",
    "    enableHiveSupport.\n",
    "    appName(s\"${username} | Spark SQL - Managing Tables - DML and Partitioning\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need to make sure that file format of the file which is being loaded into table is same as the file format used while creating the table.\n",
    "* We also need to make sure that delimiters are consistent between files and table for text file format.\n",
    "* Also data should match the criteria for the partition into which data is loaded.\n",
    "* Our `/data/retail_db/orders` have data for the whole year and hence we should not load the data directly into partition.\n",
    "* We need to split into files matching partition criteria and then load into the table.\n",
    "\n",
    "To use load command to load the files into partitions we need to pre-partition the data based on partition logic. \n",
    "\n",
    "Here is the example of using simple shell commands to partition the data. Use command prompt to run these commands\n",
    "\n",
    "\n",
    "```shell\n",
    "rm -rf ~/orders\n",
    "mkdir -p ~/orders\n",
    "\n",
    "grep 2013-07 /data/retail_db/orders/part-00000 > ~/orders/orders_201307\n",
    "grep 2013-08 /data/retail_db/orders/part-00000 > ~/orders/orders_201308\n",
    "grep 2013-09 /data/retail_db/orders/part-00000 > ~/orders/orders_201309\n",
    "grep 2013-10 /data/retail_db/orders/part-00000 > ~/orders/orders_201310\n",
    "```\n",
    "\n",
    "Let us see how we can load data into corresponding partitions. Data has to be pre-partitioned based on the partitioned column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++\n",
       "\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "USE itv002480_retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++\n",
       "\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "LOAD DATA LOCAL INPATH '/home/itv002480/orders/orders_201307'\n",
    "  INTO TABLE orders_part PARTITION (order_month=201307)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - itv002480 supergroup          0 2022-05-30 13:00 /user/itv002480/warehouse/itv002480_retail.db/orders_part/order_month=201307\n",
      "-rwxr-xr-x   3 itv002480 supergroup      64737 2022-05-30 13:00 /user/itv002480/warehouse/itv002480_retail.db/orders_part/order_month=201307/orders_201307\n",
      "drwxr-xr-x   - itv002480 supergroup          0 2022-05-30 12:45 /user/itv002480/warehouse/itv002480_retail.db/orders_part/order_month=201308\n",
      "drwxr-xr-x   - itv002480 supergroup          0 2022-05-30 12:45 /user/itv002480/warehouse/itv002480_retail.db/orders_part/order_month=201309\n",
      "drwxr-xr-x   - itv002480 supergroup          0 2022-05-30 12:45 /user/itv002480/warehouse/itv002480_retail.db/orders_part/order_month=201310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\n",
    "s\"hdfs dfs -ls -R /user/${username}/warehouse/${username}_retail.db/orders_part\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++\n",
       "\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "LOAD DATA LOCAL INPATH '/home/itv002480/orders/orders_201308'\n",
    "  INTO TABLE orders_part PARTITION (order_month=201308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++\n",
       "\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "LOAD DATA LOCAL INPATH '/home/itv002480/orders/orders_201309'\n",
    "  INTO TABLE orders_part PARTITION (order_month=201309)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++\n",
       "\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "LOAD DATA LOCAL INPATH '/home/itv002480/orders/orders_201310'\n",
    "  INTO TABLE orders_part PARTITION (order_month=201310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - itv002480 supergroup          0 2022-05-30 13:00 /user/itv002480/warehouse/itv002480_retail.db/orders_part/order_month=201307\n",
      "-rwxr-xr-x   3 itv002480 supergroup      64737 2022-05-30 13:00 /user/itv002480/warehouse/itv002480_retail.db/orders_part/order_month=201307/orders_201307\n",
      "drwxr-xr-x   - itv002480 supergroup          0 2022-05-30 13:00 /user/itv002480/warehouse/itv002480_retail.db/orders_part/order_month=201308\n",
      "-rwxr-xr-x   3 itv002480 supergroup     243190 2022-05-30 13:00 /user/itv002480/warehouse/itv002480_retail.db/orders_part/order_month=201308/orders_201308\n",
      "drwxr-xr-x   - itv002480 supergroup          0 2022-05-30 13:01 /user/itv002480/warehouse/itv002480_retail.db/orders_part/order_month=201309\n",
      "-rwxr-xr-x   3 itv002480 supergroup     251262 2022-05-30 13:01 /user/itv002480/warehouse/itv002480_retail.db/orders_part/order_month=201309/orders_201309\n",
      "drwxr-xr-x   - itv002480 supergroup          0 2022-05-30 13:01 /user/itv002480/warehouse/itv002480_retail.db/orders_part/order_month=201310\n",
      "-rwxr-xr-x   3 itv002480 supergroup     233173 2022-05-30 13:01 /user/itv002480/warehouse/itv002480_retail.db/orders_part/order_month=201310/orders_201310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\n",
    "s\"hdfs dfs -ls -R /user/${username}/warehouse/${username}_retail.db/orders_part\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETE\n",
      "67755,2013-10-30 00:00:00.0,8386,PENDING_PAYMENT\n",
      "67756,2013-10-31 00:00:00.0,6146,PROCESSING\n",
      "67757,2013-10-31 00:00:00.0,441,CLOSED\n",
      "67758,2013-10-31 00:00:00.0,6750,CLOSED\n",
      "67759,2013-10-31 00:00:00.0,10759,PENDING_PAYMENT\n",
      "68725,2013-10-01 00:00:00.0,7795,PROCESSING\n",
      "68726,2013-10-02 00:00:00.0,8817,PENDING_PAYMENT\n",
      "68727,2013-10-05 00:00:00.0,5880,PENDING_PAYMENT\n",
      "68728,2013-10-07 00:00:00.0,7267,COMPLETE\n",
      "68729,2013-10-09 00:00:00.0,8043,PENDING_PAYMENT\n",
      "68730,2013-10-11 00:00:00.0,3568,PENDING_PAYMENT\n",
      "68731,2013-10-13 00:00:00.0,8102,PENDING_PAYMENT\n",
      "68732,2013-10-14 00:00:00.0,9990,COMPLETE\n",
      "68733,2013-10-16 00:00:00.0,12429,CLOSED\n",
      "68734,2013-10-18 00:00:00.0,8510,ON_HOLD\n",
      "68735,2013-10-21 00:00:00.0,788,COMPLETE\n",
      "68736,2013-10-23 00:00:00.0,8462,COMPLETE\n",
      "68737,2013-10-26 00:00:00.0,10302,PROCESSING\n",
      "68738,2013-10-27 00:00:00.0,1100,COMPLETE\n",
      "68739,2013-10-28 00:00:00.0,2528,PENDING\n",
      "68740,2013-10-29 00:00:00.0,10691,ON_HOLD\n",
      "68741,2013-10-30 00:00:00.0,5974,PENDING_PAYMENT\n",
      "68742,2013-10-31 00:00:00.0,197,COMPLETE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\n",
    "s\"hdfs dfs -tail /user/${username}/warehouse/${username}_retail.db/orders_part/order_month=201310/orders_201310\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "|    4154|2013-...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "+--------+--------------------+-----------------+---------------+-----------+\n",
       "|order_id|          order_date|order_customer_id|   order_status|order_month|\n",
       "+--------+--------------------+-----------------+---------------+-----------+\n",
       "|    4148|2013-08-18 00:00:...|             4941|     PROCESSING|     201308|\n",
       "|    4149|2013-08-18 00:00:...|            11598|     PROCESSING|     201308|\n",
       "|    4150|2013-08-18 00:00:...|            10602|     PROCESSING|     201308|\n",
       "|    4151|2013-08-18 00:00:...|             1108|PENDING_PAYMENT|     201308|\n",
       "|    4152|2013-08-18 00:00:...|              675|       COMPLETE|     201308|\n",
       "|    4153|2013-08-18 00:00:...|             2764|PENDING_PAYMENT|     201308|\n",
       "|    4154|2013-08-18 00:00:...|             8822|PENDING_PAYMENT|     201308|\n",
       "|    4155|2013-08-18 00:00:...|            12004|         CLOSED|     201308|\n",
       "|    4156|2013-08-18 00:00:...|              733|     PROCESSING|     201308|\n",
       "|    4157|2013-08-18 00:00:...|              504|         CLOSED|     201308|\n",
       "+--------+--------------------+-----------------+---------------+-----------+\n",
       "\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM orders_part LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2 - Scala",
   "language": "scala",
   "name": "spark_2_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
